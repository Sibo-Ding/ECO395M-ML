{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from typing import List\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold the words\n",
    "word_list = []\n",
    "letters = set('abcdfeghijklmnopqrstuvwxyz')\n",
    "max_length = 32\n",
    "\n",
    "# Define a regular expression pattern to match standard punctuation\n",
    "pattern = r'[_\\W]+'\n",
    "\n",
    "# Loop through all the synsets in WordNet\n",
    "for synset in wordnet.all_synsets():\n",
    "\n",
    "    # Loop through all the lemma names for each synset\n",
    "    for lemma in synset.lemma_names():\n",
    "\n",
    "        # Split the lemma on '_' and convert to lower case\n",
    "        words = re.split(pattern, lemma.lower())\n",
    "\n",
    "        # Loop through each word in the lemma\n",
    "        for word in words:\n",
    "\n",
    "            # Remove all characters other than the 26 lower-case consonants\n",
    "            word = ''.join(filter(lambda c: c in letters, word))\n",
    "            word = word[:max_length]\n",
    "\n",
    "            # Add the word to the word list\n",
    "            if word:\n",
    "                word_list.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some words\n",
    "print(len(word_list))\n",
    "print(random.sample(word_list, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseWordDataset(Dataset):\n",
    "    def __init__(self, word_list, max_length=34):\n",
    "        self.word_list = word_list\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.word_list[idx]\n",
    "        reversed_word = word[::-1]\n",
    "        return self.encode_word(reversed_word), self.encode_word(word)\n",
    "\n",
    "    def encode_word(self, word):\n",
    "        encoded = [0]  # Start of sentence character\n",
    "        for char in word[:self.max_length-2]:  # Reserve space for start and end tokens\n",
    "            encoded.append(ord(char) - ord('a') + 1)\n",
    "        encoded.append(27)  # End of sentence character\n",
    "        while len(encoded) < self.max_length:\n",
    "            encoded.append(28)  # Padding character\n",
    "        return torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "    def decode_word(self, encoded_word):\n",
    "        decoded = []\n",
    "        for idx in encoded_word:\n",
    "            if idx == 0 or idx == 27:\n",
    "                continue\n",
    "            elif idx == 28:\n",
    "                break\n",
    "            else:\n",
    "                decoded.append(chr(idx + ord('a') - 1))\n",
    "        return ''.join(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReverseWordDataset(word_list, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev, orig = dataset.__getitem__(np.random.randint(0, 2e5))\n",
    "print(dataset.decode_word(orig))\n",
    "print(dataset.decode_word(rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_seq, target_seq = next(iter(dataloader))\n",
    "print(input_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_chars, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(num_chars, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        # One-hot encode the input\n",
    "        one_hot = torch.zeros(input_batch.size(0), input_batch.size(1), self.num_chars).to(input_batch.device)\n",
    "        one_hot.scatter_(2, input_batch.unsqueeze(2), 1)\n",
    "\n",
    "        # Encode the one-hot encoded input using a GRU\n",
    "        _, hidden = self.gru(one_hot)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "num_chars = 29  # Including start of sentence (0), end of sentence (27), and padding (28) tokens\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "\n",
    "# Create the encoder\n",
    "encoder = Encoder(num_chars, hidden_size, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hn = encoder(input_seq)\n",
    "print(hn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_chars, hidden_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(num_chars, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_chars)\n",
    "\n",
    "    def forward(self, input_step, hidden):\n",
    "        # One-hot encode the input step\n",
    "        one_hot = torch.zeros(input_step.size(0), input_step.size(1), self.num_chars).to(input_step.device)\n",
    "        one_hot.scatter_(2, input_step.unsqueeze(2), 1)\n",
    "\n",
    "        # Decode the one-hot encoded input step using a GRU\n",
    "        output, hidden = self.gru(one_hot, hidden)\n",
    "\n",
    "        # Output the logits for each character class (0-28)\n",
    "        logits = self.fc(output)\n",
    "\n",
    "        return logits, hidden\n",
    "\n",
    "# Create the decoder\n",
    "decoder = Decoder(num_chars, hidden_size, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits,_ = decoder(target_seq, hn)\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_batch, target_batch):\n",
    "        # Encode the input batch\n",
    "        hidden = self.encoder(input_batch)\n",
    "\n",
    "        # Initialize the input step with the start of sentence token (0)\n",
    "        input_step = target_batch[:, 0].unsqueeze(1)\n",
    "\n",
    "        # Decode the target batch one step at a time\n",
    "        outputs = []\n",
    "        for t in range(1, target_batch.size(1)):\n",
    "            logits, hidden = self.decoder(input_step, hidden)\n",
    "            outputs.append(logits)\n",
    "            input_step = target_batch[:, t].unsqueeze(1)\n",
    "\n",
    "        return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(encoder, decoder)\n",
    "\n",
    "print(target_seq.shape)\n",
    "out = seq2seq(input_seq, target_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Create the dataset and data loader\n",
    "reverse_word_dataset = ReverseWordDataset(word_list)\n",
    "data_loader = torch.utils.data.DataLoader(reverse_word_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(seq2seq.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1099it [09:50,  1.74it/s]"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (input_batch, target_batch) in tqdm(enumerate(data_loader)):\n",
    "        # Forward pass\n",
    "        logits = seq2seq(input_batch, target_batch)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(logits.view(-1, num_chars), target_batch[:, 1:].reshape(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_words(words, seq2seq, dataset, max_length=34):\n",
    "    # Encode the input words\n",
    "    input_batch = [dataset.encode_word(word[::-1]) for word in words]\n",
    "\n",
    "    # Create a tensor with the encoded words\n",
    "    input_batch = torch.stack(input_batch)\n",
    "\n",
    "    # Initialize the input step with the start of sentence token (0)\n",
    "    input_step = torch.zeros(input_batch.size(0), 1, dtype=torch.long)\n",
    "\n",
    "    # Initialize the hidden state with the output from the encoder\n",
    "    hidden = seq2seq.encoder(input_batch)\n",
    "\n",
    "    # Decode the target sequence one character at a time\n",
    "    decoded_words = []\n",
    "    for _ in range(max_length):\n",
    "        logits, hidden = seq2seq.decoder(input_step, hidden)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        decoded_words.append(predictions.squeeze(1))\n",
    "        input_step = predictions\n",
    "\n",
    "    # Transpose the list of decoded words\n",
    "    decoded_words = torch.stack(decoded_words).transpose(0, 1)\n",
    "\n",
    "    # Turn the predicted target sequences back into strings\n",
    "    reversed_words = [dataset.decode_word(encoded_word) for encoded_word in decoded_words]\n",
    "\n",
    "    return reversed_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['elppa', 'ananab', 'eparg', 'egnaro', 'yrrebwarts']\n",
      "Target: ['apple', 'banana', 'grape', 'orange', 'strawberry']\n",
      "Predicted: ['share', 'sharis', 'shart', 'sharil', 'conthoria']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_words = [\"apple\", \"banana\", \"grape\", \"orange\", \"strawberry\"]\n",
    "input_words_rev = [word[::-1] for word in input_words]\n",
    "reversed_words = reverse_words(input_words_rev, seq2seq, reverse_word_dataset)\n",
    "\n",
    "\n",
    "print(f\"Input: {input_words_rev}\")\n",
    "print(f\"Target: {input_words}\")\n",
    "print(f\"Predicted: {reversed_words}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
